{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5ed5dc6",
   "metadata": {},
   "source": [
    "# LangChain + Ollama: Prompt Template Lab\n",
    "\n",
    "This notebook demonstrates how to use prompt templates with a locally running Ollama model. You'll learn how to create, format, and use prompts to get more accurate and relevant responses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61b237d",
   "metadata": {},
   "source": [
    "# Install Required Packages\n",
    "To use LangChain with Ollama, you need to install the following Python packages:\n",
    "- `ollama`: Python client for locally running Ollama models\n",
    "- `langchain_community`: Community-contributed LangChain tools\n",
    "- `python-dotenv`: For loading environment variables from a `.env` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defc8c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b64937",
   "metadata": {},
   "source": [
    "# Import Libraries\n",
    "We need to import the following libraries:\n",
    "- `os`: For accessing environment variables\n",
    "- `load_dotenv` from `dotenv`: To load variables from a `.env` file\n",
    "- `ollama`: To interact with Ollama models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32903db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75885ad6",
   "metadata": {},
   "source": [
    "# Load Environment Variables\n",
    "Environment variables are used to securely store sensitive information. We use `load_dotenv()` to load these variables from a `.env` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8c795a",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "ollama_model = os.getenv(\"OLLAMA_MODEL\", \"llama2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1a5a1f",
   "metadata": {},
   "source": [
    "# Create a Prompt Template\n",
    "A prompt template allows you to format your prompt dynamically. Here, we'll use a template to generate a poem about AI and coffee."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339989f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Write a short poem about AI and coffee.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777f6497",
   "metadata": {},
   "source": [
    "# Pull the Model (if not already available)\n",
    "We use `ollama.pull()` to ensure the model is available locally before invoking it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f4c0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    ollama.pull(ollama_model)\n",
    "except Exception as e:\n",
    "    print(f\"Error pulling model: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f261d8",
   "metadata": {},
   "source": [
    "# Invoke the Language Model\n",
    "We use the `ollama.chat()` method to send the prompt to the language model and get a response. The model processes the prompt and generates an answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e514f031",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = ollama.chat(model=ollama_model, messages=[{\"role\": \"user\", \"content\": prompt}])\n",
    "print(\"Response:\", response[\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b301f19",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "*Try changing the prompt template to experiment with different outputs!*"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
