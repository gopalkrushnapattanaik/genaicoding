{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d54117cd",
   "metadata": {},
   "source": [
    "# LangChain + Ollama: Temperature Effects Lab\n",
    "\n",
    "This notebook demonstrates how temperature affects the creativity and randomness of responses from a locally running Ollama model. You'll learn how to set up the environment, send prompts, and interpret the model's output at different temperature settings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48fb6d5",
   "metadata": {},
   "source": [
    "# Install Required Packages\n",
    "To use LangChain with Ollama, you need to install the following Python packages:\n",
    "- `ollama`: Python client for locally running Ollama models\n",
    "- `langchain_community`: Community-contributed LangChain tools\n",
    "- `python-dotenv`: For loading environment variables from a `.env` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea59713a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143b7dc1",
   "metadata": {},
   "source": [
    "# Import Libraries\n",
    "We need to import the following libraries:\n",
    "- `os`: For accessing environment variables\n",
    "- `load_dotenv` from `dotenv`: To load variables from a `.env` file\n",
    "- `ollama`: To interact with Ollama models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f2f2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a015e748",
   "metadata": {},
   "source": [
    "# Load Environment Variables\n",
    "Environment variables are used to securely store sensitive information. We use `load_dotenv()` to load these variables from a `.env` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af6b5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "ollama_model = os.getenv(\"OLLAMA_MODEL\", \"llama2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ee0a3b",
   "metadata": {},
   "source": [
    "# Create a Prompt\n",
    "A prompt is a question or instruction you send to the language model. Here, we'll ask about temperature effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f71f05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Explain the effect of temperature on LLM creativity.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d650a4",
   "metadata": {},
   "source": [
    "# Pull the Model (if not already available)\n",
    "We use `ollama.pull()` to ensure the model is available locally before invoking it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621a968a",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    ollama.pull(ollama_model)\n",
    "except Exception as e:\n",
    "    print(f\"Error pulling model: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b138265",
   "metadata": {},
   "source": [
    "# Invoke the Language Model\n",
    "We use the `ollama.chat()` method to send the prompt to the language model and get a response. The model processes the prompt and generates an answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48bea2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = ollama.chat(model=ollama_model, messages=[{\"role\": \"user\", \"content\": prompt}])\n",
    "print(\"Response:\", response[\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ada283",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "*Try changing the prompt or temperature settings to see how the model's creativity changes!*"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
