{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbfe6328",
   "metadata": {},
   "source": [
    "# LangChain + Ollama: Resume RAG Agent Lab\n",
    "\n",
    "This notebook demonstrates how to use Retrieval Augmented Generation (RAG) with a locally running Ollama model to answer interview questions based on your resume. You'll learn how to load a resume PDF, split it into chunks, and use Ollama to answer questions using the resume as context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6d8fde",
   "metadata": {},
   "source": [
    "# Install Required Packages\n",
    "To use LangChain with Ollama, you need to install the following Python packages:\n",
    "- `ollama`: Python client for locally running Ollama models\n",
    "- `langchain_community`: Community-contributed LangChain tools\n",
    "- `python-dotenv`: For loading environment variables from a `.env` file\n",
    "- `faiss-cpu`: For vector search\n",
    "- `pypdf`: For PDF loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899dcb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e9dea6",
   "metadata": {},
   "source": [
    "# Import Libraries\n",
    "We need to import the following libraries:\n",
    "- `os`: For accessing environment variables\n",
    "- `load_dotenv` from `dotenv`: To load variables from a `.env` file\n",
    "- `ollama`: To interact with Ollama models\n",
    "- `PyPDFLoader` from `langchain_community.document_loaders`: To load PDF\n",
    "- `FAISS` from `langchain_community.vectorstores`: For vector search\n",
    "- `RecursiveCharacterTextSplitter` from `langchain.text_splitter`: For chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f060eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import ollama\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367cb579",
   "metadata": {},
   "source": [
    "# Load Environment Variables\n",
    "Environment variables are used to securely store sensitive information. We use `load_dotenv()` to load these variables from a `.env` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684c06ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "ollama_model = os.getenv(\"OLLAMA_MODEL\", \"llama2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed69d4dd",
   "metadata": {},
   "source": [
    "# Load and Chunk Resume PDF\n",
    "Load your resume PDF and split it into manageable chunks for semantic search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6dd95c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = \"my_resume.pdf\"\n",
    "loader = PyPDFLoader(pdf_path)\n",
    "docs = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "split_docs = text_splitter.split_documents(docs)\n",
    "vectorstore = FAISS.from_documents(split_docs, embedding=None)\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50ac18e",
   "metadata": {},
   "source": [
    "# Ask Interview Questions\n",
    "Send a question to the model using the resume context. If the answer is not found, the model will say so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0b17e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_resume_question(question):\n",
    "    docs = retriever.get_relevant_documents(question)\n",
    "    context = \"\\n\".join([doc.page_content for doc in docs])\n",
    "    prompt = f\"Answer the following interview question using only the resume context below. If not found, say 'I don't know based on my resume.'\\nResume:\\n{context}\\nQuestion: {question}\"\n",
    "    response = ollama.chat(model=ollama_model, messages=[{\"role\": \"user\", \"content\": prompt}])\n",
    "    return response[\"message\"][\"content\"]\n",
    "\n",
    "question = \"What are your key skills?\"\n",
    "print(\"Bot:\", ask_resume_question(question))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86e58be",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "*Try changing the question to see how the model answers using your resume!*"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
