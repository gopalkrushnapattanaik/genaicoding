{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82d5af63",
   "metadata": {},
   "source": [
    "# LangChain + Ollama: Instructions & Concepts\n",
    "\n",
    "This notebook provides a structured overview and step-by-step instructions for using LangChain with locally running Ollama models. It covers environment setup, prompt engineering, RAG, agent tools, and integration with external APIsâ€”all with clear notes and code samples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0bedf95",
   "metadata": {},
   "source": [
    "## 1. Environment Setup\n",
    "- Install Python dependencies using `requirements.txt`.\n",
    "- Ensure Ollama is installed and running locally.\n",
    "- Download the desired model (e.g., `llama2`) using `ollama pull llama2`.\n",
    "- Set up your `.env` file with any required variables (e.g., `OLLAMA_MODEL=llama2`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a36f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11de8df1",
   "metadata": {},
   "source": [
    "## 2. Basic Prompting with Ollama\n",
    "Send a simple prompt to the local model and print the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81232375",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import ollama\n",
    "load_dotenv()\n",
    "ollama_model = os.getenv(\"OLLAMA_MODEL\", \"llama2\")\n",
    "prompt = \"What is the capital of France?\"\n",
    "response = ollama.chat(model=ollama_model, messages=[{\"role\": \"user\", \"content\": prompt}])\n",
    "print(\"Response:\", response[\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42023d8d",
   "metadata": {},
   "source": [
    "## 3. Prompt Templates\n",
    "Format prompts dynamically for creative tasks (e.g., poetry)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46316994",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Write a short poem about AI and coffee.\"\n",
    "response = ollama.chat(model=ollama_model, messages=[{\"role\": \"user\", \"content\": prompt}])\n",
    "print(\"Response:\", response[\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be64a573",
   "metadata": {},
   "source": [
    "## 4. RAG (Retrieval Augmented Generation)\n",
    "Load a resume PDF, split into chunks, and answer questions using the resume as context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6583ef58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "pdf_path = \"my_resume.pdf\"\n",
    "loader = PyPDFLoader(pdf_path)\n",
    "docs = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "split_docs = text_splitter.split_documents(docs)\n",
    "vectorstore = FAISS.from_documents(split_docs, embedding=None)\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4})\n",
    "def ask_resume_question(question):\n",
    "    docs = retriever.get_relevant_documents(question)\n",
    "    context = \"\\n\".join([doc.page_content for doc in docs])\n",
    "    prompt = f\"Answer the following interview question using only the resume context below. If not found, say 'I don't know based on my resume.'\\nResume:\\n{context}\\nQuestion: {question}\"\n",
    "    response = ollama.chat(model=ollama_model, messages=[{\"role\": \"user\", \"content\": prompt}])\n",
    "    return response[\"message\"][\"content\"]\n",
    "question = \"What are your key skills?\"\n",
    "print(\"Bot:\", ask_resume_question(question))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc318f0",
   "metadata": {},
   "source": [
    "## 5. Agent Tools & External API Integration\n",
    "Use prompt engineering to interact with APIs (e.g., Google Search, Slack, Weather)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac5a51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Search Google for the latest news about AI.\"\n",
    "response = ollama.chat(model=ollama_model, messages=[{\"role\": \"user\", \"content\": prompt}])\n",
    "print(\"Response:\", response[\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7538ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Send today's Bangalore weather update to Slack.\"\n",
    "response = ollama.chat(model=ollama_model, messages=[{\"role\": \"user\", \"content\": prompt}])\n",
    "print(\"Response:\", response[\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "facc28a4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "This notebook covers all major concepts and sample code for using LangChain with Ollama. For more details, see the individual lab notebooks in this folder."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
