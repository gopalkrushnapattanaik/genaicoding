{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8823e19",
   "metadata": {},
   "source": [
    "# LangChain + Ollama: RAG Lab\n",
    "\n",
    "This notebook demonstrates how to use Retrieval Augmented Generation (RAG) with a locally running Ollama model. You'll learn how to set up the environment, send prompts, and interpret the model's output for RAG tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7254a469",
   "metadata": {},
   "source": [
    "# Install Required Packages\n",
    "To use LangChain with Ollama, you need to install the following Python packages:\n",
    "- `ollama`: Python client for locally running Ollama models\n",
    "- `langchain_community`: Community-contributed LangChain tools\n",
    "- `python-dotenv`: For loading environment variables from a `.env` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66af88b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c371ab",
   "metadata": {},
   "source": [
    "# Import Libraries\n",
    "We need to import the following libraries:\n",
    "- `os`: For accessing environment variables\n",
    "- `load_dotenv` from `dotenv`: To load variables from a `.env` file\n",
    "- `ollama`: To interact with Ollama models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0652c43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a7e9ac",
   "metadata": {},
   "source": [
    "# Load Environment Variables\n",
    "Environment variables are used to securely store sensitive information. We use `load_dotenv()` to load these variables from a `.env` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09480c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "ollama_model = os.getenv(\"OLLAMA_MODEL\", \"llama2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128d1ee0",
   "metadata": {},
   "source": [
    "# Create a RAG Prompt\n",
    "A prompt is a question or instruction you send to the language model. Here, we'll ask about Retrieval Augmented Generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5bb44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Summarize the concept of Retrieval Augmented Generation (RAG).\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191b13b5",
   "metadata": {},
   "source": [
    "# Pull the Model (if not already available)\n",
    "We use `ollama.pull()` to ensure the model is available locally before invoking it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b0f030",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    ollama.pull(ollama_model)\n",
    "except Exception as e:\n",
    "    print(f\"Error pulling model: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb3475a",
   "metadata": {},
   "source": [
    "# Invoke the Language Model\n",
    "We use the `ollama.chat()` method to send the prompt to the language model and get a response. The model processes the prompt and generates an answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec155ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = ollama.chat(model=ollama_model, messages=[{\"role\": \"user\", \"content\": prompt}])\n",
    "print(\"Response:\", response[\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209cf4fd",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "*Try changing the RAG prompt to experiment with different retrieval tasks!*"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
