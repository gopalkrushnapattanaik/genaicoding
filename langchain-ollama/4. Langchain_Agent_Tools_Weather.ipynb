{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0696a9e0",
   "metadata": {},
   "source": [
    "# LangChain + Ollama: Agent Tools Weather Lab\n",
    "\n",
    "This notebook demonstrates how to use a locally running Ollama model to answer weather-related questions. You'll learn how to set up the environment, send prompts, and interpret the model's output for weather queries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8d8cfa",
   "metadata": {},
   "source": [
    "# Install Required Packages\n",
    "To use LangChain with Ollama, you need to install the following Python packages:\n",
    "- `ollama`: Python client for locally running Ollama models\n",
    "- `langchain_community`: Community-contributed LangChain tools\n",
    "- `python-dotenv`: For loading environment variables from a `.env` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c6bb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56b952b",
   "metadata": {},
   "source": [
    "# Import Libraries\n",
    "We need to import the following libraries:\n",
    "- `os`: For accessing environment variables\n",
    "- `load_dotenv` from `dotenv`: To load variables from a `.env` file\n",
    "- `ollama`: To interact with Ollama models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c99362",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ccd562",
   "metadata": {},
   "source": [
    "# Load Environment Variables\n",
    "Environment variables are used to securely store sensitive information. We use `load_dotenv()` to load these variables from a `.env` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4762c615",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "ollama_model = os.getenv(\"OLLAMA_MODEL\", \"llama2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c6e31a",
   "metadata": {},
   "source": [
    "# Create a Weather Prompt\n",
    "A prompt is a question or instruction you send to the language model. Here, we'll ask about the weather in Bangalore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24e6425",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"What is the weather in Bangalore today?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711c1a70",
   "metadata": {},
   "source": [
    "# Pull the Model (if not already available)\n",
    "We use `ollama.pull()` to ensure the model is available locally before invoking it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df00a3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    ollama.pull(ollama_model)\n",
    "except Exception as e:\n",
    "    print(f\"Error pulling model: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74987c8",
   "metadata": {},
   "source": [
    "# Invoke the Language Model\n",
    "We use the `ollama.chat()` method to send the prompt to the language model and get a response. The model processes the prompt and generates an answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c4d0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = ollama.chat(model=ollama_model, messages=[{\"role\": \"user\", \"content\": prompt}])\n",
    "print(\"Response:\", response[\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f33d5f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "*Try changing the location or weather prompt to see different responses!*"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
