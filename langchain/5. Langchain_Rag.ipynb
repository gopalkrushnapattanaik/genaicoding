{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea1ac95a",
   "metadata": {},
   "source": [
    "# LangChain RAG Lab: Resume Q&A with Robo1! ðŸ“„ðŸ¤–\n",
    "\n",
    "Welcome to the LangChain RAG (Retrieval-Augmented Generation) Lab! In this notebook, you'll build a chatbot that answers interview questions using your resume. Robo1 is here to helpâ€”he loves finding answers in big piles of documents!\n",
    "\n",
    "<table><tr>\n",
    "<td><img src=\"images/robo1.png\" alt=\"Robo1 - RAG Expert\" width=\"120\" /></td>\n",
    "<td style=\"vertical-align:top; padding-left:20px;\">\n",
    "<b>Robo1 says:</b><br>\n",
    "<i>\"I can search your resume faster than you can say 'curriculum vitae'!\"</i><br>\n",
    "</td>\n",
    "</tr></table>\n",
    "\n",
    "Let's get started and see how RAG makes chatbots smarter!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c393d81",
   "metadata": {},
   "source": [
    "# What is RAG (Retrieval-Augmented Generation)?\n",
    "RAG combines the power of search (retrieval) with language models (generation). Instead of guessing, your chatbot can look up answers in documentsâ€”like your resume!\n",
    "\n",
    "Robo1 loves RAG because it means less guessing and more knowing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534c7201",
   "metadata": {},
   "source": [
    "# Import Libraries and Setup\n",
    "Let's import the necessary libraries and set up our environment for RAG-powered resume Q&A."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c42cb6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import OpenAI\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get OpenAI API key from environment\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e331ea21",
   "metadata": {},
   "source": [
    "# Load and Chunk Your Resume\n",
    "Let's load your resume PDF and split it into smaller chunks so Robo1 can search it efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19025a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = \"my_resume.pdf\"\n",
    "loader = PyPDFLoader(pdf_path)\n",
    "docs = loader.load()\n",
    "\n",
    "# Split resume into smaller chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "split_docs = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d14705",
   "metadata": {},
   "source": [
    "### Create a Vector Store and Retriever\n",
    "We'll turn your resume chunks into searchable vectors so Robo1 can find the best answers fast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6105f5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = FAISS.from_documents(split_docs, embeddings)\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c42dc01d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents in FAISS vectorstore: 7\n",
      "Document IDs: ['fb5ddb15-d45e-4e63-8a64-6c9cb6e52d7d', '3f117df1-43e0-43da-8cee-d2d4b65732a3', '342fba6e-6ae1-4f66-bad8-fcedee3e6ad5', '068c39c6-081b-4c7b-bafc-5a727c85213a', 'eab656fd-2b42-4595-a3e0-5780c174c397']\n",
      "Sample document content:\n",
      " K e y  S k i l l s\n",
      "A b o u t  m e\n",
      "      + 9 1 - 8 8 0 0 1 7 5 5 4 4\n",
      "        n a v d e e p 2 2 0 1 6 1 @ g m a i l . c o m\n",
      "N a v d e e p  K a u r\n",
      "C o r p o r a t e   T r a i n e r  ( D a t a / C l o u d / M L  E n g i n e e r i n g )\n",
      "P r e m i u m  I n s t r u c t o r  w i t h  1 4 +  y e a r s  o f  i n d u s t r y\n",
      "e x p e r i e n c e  i n  d i f f e r e n t  d o m a i n s  &  m u l t i p l e\n",
      "t e c h n o l o g i e s . S u c c e s s f u l l y  d e s i g n e d  a n d  c o n d u c t e d\n",
      "o v e r  2 5 0 +  h a n d s - o n ,  r e a l - w o r l d  t r a i n i n g  s e s s i o n s  f o r\n",
      "r e p u t e d  M N C s ,  f o c u s i n g  o n  B i g  D a t a ,  C l o u d ,  D e v O p s ,\n",
      "a n d  A I  t e c h n o l o g i e s .\n",
      "T R A I N I N G  E X P E R I E N C E  \n",
      "B i g  D a t a  T e c h n o l o g i e s :  C o m p r e h e n s i v e  t r a i n i n g  o n  A p a c h e\n",
      "S p a r k ,  H a d o o p ,  C o n f l u e n t  K a f k a ,  a n d  A p a c h e  H u d i / I c e b e r g ,\n",
      "i n c l u d i n g  r e a l - t i m e  d a t a  p r o c e s s i n g ,  b a t c h  p r o c e s s i n g ,  a n d\n",
      "p e r f o r m a n c e  o p t i m i z a t i o n  t e c h n i q u e s .\n",
      "O r c h e s t r a t i o n  &  D a t a  P i p e l i n e s :  D e l i v e r e d  t r a i n i n g  t o  c r e a t e  e n d - t o -\n",
      "e n d  w o r k f l o w  o r c h e s t r a t i o n  u s i n g  A p a c h e  A i r f l o w  ( A s t r o n o m e r )  a n d\n",
      "d a t a  p i p e l i n e  d e s i g n  w i t h  A p a c h e  B e a m .\n",
      "C l o u d  E c o s y s t e m s :\n",
      "A W S  C l o u d :  H a n d s - o n  l a b s  f o r  A W S  f u n d m e n t a l  S e r v i c e s ,  A W S  M L\n",
      "w i t h  S a g e m a k e r ,  A W S  D a t a  E n g i n e e r i n g  s e r v i c e s  , G e n A I  u s i n g  A W S\n",
      "b e d r o c k ,  A W S  I n f r a s t r u c t u r e  s e r v i c e s  -  c l o u d f o r m a t i o n  e t c .\n",
      "G o o g l e  C l o u d  P l a t f o r m :   B i g Q u e r y ,  G K E ,  D a t a f l o w ,  D a t a p r o c  e t c\n",
      "FAISS index shape: 7 vectors, 1536 dimensions\n",
      "Sample vector (first 10 values): [-0.02790977  0.00159834  0.0219361  -0.01509506 -0.00433686  0.02641286\n",
      " -0.01699768 -0.02326514 -0.01636814 -0.02513978]\n"
     ]
    }
   ],
   "source": [
    "# Display information about the FAISS vector store\n",
    "print(\"Number of documents in FAISS vectorstore:\", len(vectorstore.docstore._dict))\n",
    "print(\"Document IDs:\", list(vectorstore.docstore._dict.keys())[:5])  # Show first 5 IDs\n",
    "\n",
    "# Optionally, show a sample document stored in FAISS\n",
    "sample_id = list(vectorstore.docstore._dict.keys())[0]\n",
    "print(\"Sample document content:\\n\", vectorstore.docstore._dict[sample_id].page_content)\n",
    "\n",
    "# Show vector representation details\n",
    "print(\"FAISS index shape:\", vectorstore.index.ntotal, \"vectors,\", vectorstore.index.d, \"dimensions\")\n",
    "sample_vector = vectorstore.index.reconstruct(0)\n",
    "print(\"Sample vector (first 10 values):\", sample_vector[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d84830",
   "metadata": {},
   "source": [
    "### Build the RAG Chatbot Chain\n",
    "Now we'll connect everything together so Robo1 can answer your interview questions using your resume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4f894957",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(api_key=openai_api_key)\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "You are answering interview questions based on the following resume context.\n",
    "If the answer is not in the context, say \\\"I don't know based on my resume.\\\"\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "Answer as if you are the candidate:\n",
    "\"\"\"\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    return_source_documents=False,\n",
    "    chain_type_kwargs={\"prompt\": PromptTemplate.from_template(prompt_template)}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a499dc",
   "metadata": {},
   "source": [
    "# Chat with Your Resume!\n",
    "Ask interview questions and see how Robo1 answers using your resume. If he can't find the answer, he'll let you know!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cf279012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: {'query': 'What are your technical skills?', 'result': '\\nBased on my resume, my technical skills include experience in managing and training teams on various technologies, creating robust big data pipelines, ensuring best and secure practices, and migrating teams from on-premise to cloud on Kubernetes. I also have extensive experience in Big Data technologies such as Apache Spark, Hadoop, Confluent Kafka, and Apache Hudi/Iceberg. In addition, I have training experience in cloud ecosystems such as AWS and Google Cloud, as well as in NoSQL databases like Cassandra, Neo4J, and MongoDB. I am also familiar with microservices, containerization, orchestration, and CI/CD pipelines. Other skills include data warehousing and integration, machine learning operations, and AI technologies such as GenaAI, Apache Spark/PySpark, and Apache Kafka. My programming languages include Scala, Python, and Java. '}\n"
     ]
    }
   ],
   "source": [
    "# Ask your interview question here\n",
    "question = \"What are your technical skills?\"\n",
    "answer = qa_chain.invoke({\"query\": question})\n",
    "print(\"Bot:\", answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68eed6e7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<table><tr>\n",
    "<td><img src=\"images/robo1.png\" alt=\"Robo1 - RAG Expert\" width=\"120\" /></td>\n",
    "<td style=\"vertical-align:top; padding-left:20px;\">\n",
    "<b>Robo1 says:</b><br>\n",
    "<i>\"Congratulations! You just built a RAG-powered chatbot. Now I can ace any interviewâ€”unless they ask about my favorite pizza topping!\"</i><br>\n",
    "<i>With RAG, your AI can find answers in documents faster than ever. ðŸ“„ðŸ¤–</i>\n",
    "</td>\n",
    "</tr></table>\n",
    "\n",
    "*Thanks for completing the LangChain RAG Lab!*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
