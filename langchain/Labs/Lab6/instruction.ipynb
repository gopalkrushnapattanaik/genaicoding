{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eafd89dd",
   "metadata": {},
   "source": [
    "# Lab 6: Working with Local Models\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this lab, you will learn how to work with local language models (LLMs) using Ollama. Ollama is a tool that allows you to run and manage open-source LLMs directly on your machine, without relying on cloud APIs. You will explore how to download, set up, and interact with these models locally, enabling private and efficient AI workflows.\n",
    "\n",
    "By the end of this lab, you will:\n",
    "- Understand the basics of Ollama\n",
    "- Download and run an LLM locally\n",
    "- Interact with the model for various tasks\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6838889d",
   "metadata": {},
   "source": [
    "### 1. Install Ollama\n",
    "\n",
    "**On macOS**\n",
    "```bash\n",
    "curl -fsSL https://ollama.com/download/Ollama-darwin.zip -o Ollama-darwin.zip\n",
    "unzip Ollama-darwin.zip -d /Applications\n",
    "```\n",
    "Or simply download from [ollama.com](https://ollama.com/download) and drag into Applications.\n",
    "\n",
    "**On Linux**\n",
    "```bash\n",
    "curl -fsSL https://ollama.com/install.sh | sh\n",
    "```\n",
    "\n",
    "**On Windows (Preview)**\n",
    "- Go to [Ollama downloads](https://ollama.com/download).\n",
    "- Download the Windows installer (`OllamaSetup.exe`).\n",
    "- Run the installer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57bbf54",
   "metadata": {},
   "source": [
    "### 2. Verify Installation\n",
    "\n",
    "After installation, check the version:\n",
    "\n",
    "```bash\n",
    "ollama --version\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72e19ff",
   "metadata": {},
   "source": [
    "### 3. List Models\n",
    "\n",
    "To list all available models:\n",
    "\n",
    "```bash\n",
    "ollama list\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b703501",
   "metadata": {},
   "source": [
    "### 4. Pull a model\n",
    "\n",
    "\n",
    "```bash\n",
    "ollama pull llama2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70c97a3",
   "metadata": {},
   "source": [
    "### 5. Run a model\n",
    "\n",
    "\n",
    "```bash\n",
    "ollama run llama2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254284d1",
   "metadata": {},
   "source": [
    "### 6. Run a Prompt\n",
    "\n",
    "When prompted, type:\n",
    "\n",
    "```\n",
    "what is capital of france?\n",
    "```\n",
    "\n",
    "The model will respond with the answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a5a79b",
   "metadata": {},
   "source": [
    "### 7. Show Available Commands\n",
    "\n",
    "To see all available commands and options for Ollama, use:\n",
    "\n",
    "```bash\n",
    "ollama help\n",
    "```\n",
    "\n",
    "This will display a list of commands you can use to manage models, run inference, and more."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e4ca5e",
   "metadata": {},
   "source": [
    "### Other Models You Can Try\n",
    "\n",
    "You can run other models with Ollama using similar commands:\n",
    "\n",
    "```bash\n",
    "ollama run mistral\n",
    "ollama run codellama\n",
    "ollama run gemma\n",
    "ollama run phi3\n",
    "ollama run llama3:70b  # (larger version, needs strong GPU)\n",
    "```\n",
    "\n",
    "ðŸ‘‰ Each `ollama run <model>` command will pull the model first (like Docker images)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51c5b7b",
   "metadata": {},
   "source": [
    "### 9. Verify Ollama Server is Running\n",
    "\n",
    "After starting Ollama, it runs a local server at [http://localhost:11434](http://localhost:11434).\n",
    "\n",
    "You can check if it's running by opening this URL in your browser or using:\n",
    "\n",
    "```bash\n",
    "curl http://localhost:11434\n",
    "```\n",
    "\n",
    "If Ollama is running, you should see a response from the server."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439b6521",
   "metadata": {},
   "source": [
    "### Use Ollama in LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0294f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain.schema import HumanMessage,SystemMessage\n",
    "\n",
    "# Connect to Ollama model\n",
    "llm = ChatOllama(model=\"mistral\",streaming=False)\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a strict Python teacher.\"),\n",
    "    HumanMessage(content=\"Explain async vs multithreading in simple words\")\n",
    "]\n",
    "\n",
    "# Send a message\n",
    "response = llm.invoke(messages)\n",
    "print(response.content)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
